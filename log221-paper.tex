\documentclass[a4paper]{article}
\usepackage[margin=1in]{geometry}

\usepackage{preamble}
\usepackage{log221-paper-macros}
\usepackage{showkeys}

\title{Curry-Howard correspondence and proof simplification}
\author{Frank Tsai}

\begin{document}

\maketitle

\begin{abstract}
  The Curry-Howard correspondence is the observation that proof systems and computational calculi, such as $\lambda$-calculi, are closely related.
  This correspondence enables us to apply programming language theory techniques to study proof systems and \viceversa.
  We introduce \emph{Tait's method of computability}, also known as \emph{logical relation}, to show that every intuitionistic natural deduction derivation can be fully simplified.
\end{abstract}

\section{Introduction}
\label{sec:0000}

\comment{
  \begin{enumerate}
  \item A brief introduction of typed $\lambda$-calculus.
  \item A brief description of the Curry-Howard correspondence.
  \item A brief description of detour conversion and the normalization problem.
  \item A brief description of Tait's computability.
  \end{enumerate}
}

The Curry-Howard correspondence establishes a translation between derivations and typed $\lambda$-terms.
Under such a correspondence proof identifications become equational axioms in the $\lambda$-calculus; moreover, detour conversions give the $\lambda$-calculus computational meaning.

It is almost a given that we can convert every derivation to a detour-free derivation; in fact, the reader probably already has an algorithm in mind, yet the standard proof-by-induction argument fails.
The induction hypothesis is too weak to handle the elimination rules.

Tait's method of computability strengthens the induction hypothesis by attaching additional semantic information to the syntax.


\section{Curry-Howard correspondence}
\label{sec:0001}

The Curry-Howard correspondence was first observed by Curry \cite{Curry58} in the setting of combinatory logic: combinators are representations of proofs of certain propositions.
The general correspondence between propositions and types was later explicated by Howard \cite{Howard80}.

The correspondence follows closely the BHK interpretation of intuitionistic logic.
For example, proof of an implication $A \wedge B$ consists of a proof of $A$ and a proof of $B$.
When we replace ``proof'' with ``$\lambda$-term,'' the resulting type is the usual product type.

\begin{figure}
  \centering
  \begin{mathpar}
    \ebrule[\rVar]{
      \hypo{x : A \in \Gamma}
      \infer1{\Gamma \vdash x : A}
    }\and
    \ebrule[\rEmpE]{
      \hypo{\Gamma \vdash e : \tpEmp}
      \infer1{\Gamma \vdash \tmAbort{e} : A}
    }\and
    \ebrule[\rProdI]{
      \hypo{\Gamma \vdash a : A}
      \hypo{\Gamma \vdash b : B}
      \infer2{\Gamma \vdash \tmPair{a}{b} : A \times B}
    }\and
    \ebrule[\rProdEl]{
      \hypo{\Gamma \vdash p : A \times B}
      \infer1{\Gamma \vdash \tmPrl{p} : A}
    }\and
    \ebrule[\rProdEr]{
      \hypo{\Gamma \vdash p : A \times B}
      \infer1{\Gamma \vdash \tmPrr{p} : B}
    }\and
    \ebrule[\rSumIl]{
      \hypo{\Gamma \vdash a : A}
      \infer1{\Gamma \vdash \tmInl{a} : A + B}
    }\and
    \ebrule[\rSumIr]{
      \hypo{\Gamma \vdash b : B}
      \infer1{\Gamma \vdash \tmInr{b} : A + B}
    }\and
    \ebrule[\rSumE]{
      \hypo{\Gamma \vdash s : A + B}
      \hypo{\Gamma, x : A \vdash c : C}
      \hypo{\Gamma, y : B \vdash c' : C}
      \infer3{\Gamma \vdash \tmCase{s}{c}{c'} : C}
    }\and
    \ebrule[\rFunI]{
      \hypo{\Gamma, x : A \vdash b : B}
      \infer1{\Gamma \vdash \tmLam{x : A}{b} : A \to B}
    }\and
    \ebrule[\rFunE]{
      \hypo{\Gamma \vdash f : A \to B}
      \hypo{\Gamma \vdash a : A}
      \infer2{\Gamma \vdash \tmApp{f}{a} : B}
    }
  \end{mathpar}
  \caption{Typing rules}
  \label{fig:0000}
\end{figure}

\begin{figure}
  \centering
  \begin{mathpar}
    \tmPrl{\tmPair{a}{b}} = a\and
    \tmPrr{\tmPair{a}{b}} = b\and
    \tmCase{\tmInl{a}}{c}{c'} = c[a/x]\and
    \tmCase{\tmInr{b}}{c}{c'} = c'[b/y]\and
    \tmApp{\tmLam{x : A}{b}}{a} = b[a/x]\and
    % \tmPrl{\tmAbort{e}} = \tmAbort{e}\and
    % \tmPrr{\tmAbort{e}} = \tmAbort{e}\and
    % \tmCase{\tmAbort{e}}{c}{c'} = \tmAbort{e}\and
    % \tmApp{\tmAbort{e}}{a} = \tmAbort{e}
  \end{mathpar}
  \caption{Equational axioms}
  \label{fig:0001}
\end{figure}

\subsection{From proofs to programs}
\label{sec:0002}



\subsection{From programs to proofs}
\label{sec:0003}

\subsection{Detour conversion and $\beta$-reduction}
\label{sec:0005}

\begin{mathpar}
  \ebrule{
    \hypo{[A]}
    \ellipsis{$\cD_2$}{B}
    \infer1[\rFunI]{A \to B}
    \hypo{}
    \ellipsis{$\cD_1$}{A}
    \infer2[\rFunE]{B}
  }\qquad=_{\beta}\qquad
  \ebrule{
    \hypo{}
    \ellipsis{$\cD_1$}{A}
    \ellipsis{$\cD_2$}{B}
  }\and
  \ebrule{
    \hypo{}
    \ellipsis{$\cD_1$}{A}
    \hypo{}
    \ellipsis{$\cD_2$}{B}
    \infer2[\rProdI]{A \wedge B}
    \infer1[\rProdEl]{A}
  }\qquad=_{\beta}\qquad
  \ebrule{
    \hypo{}
    \ellipsis{$\cD_1$}{A}
  }
\end{mathpar}

\subsection{Normalization}
\label{sec:0004}

\begin{theorem}
  If $\Gamma \vdash t : A$, then $t$ is normalizing.
\end{theorem}
\begin{proof}
  We proceed by induction on the typing derivation $\Gamma \vdash t : A$.
  \begin{itemize}
  \item[\rVar:] Variables are irreducible.
  \item[\rProdI:] We must show that $\tmPair{b}{c}\tpAnno{B \times C}$ is normalizing.
    By the induction hypothesis, $b\tpAnno{B}$ and $c\tpAnno{C}$ are normalizing.
    Let $b'$ and $c'$ be the normal forms of $b$ and $c$ respectively; then $\tmPair{b'}{c'}$ is irreducible and $\tmPair{b}{c} \bRed* \tmPair{b'}{c'}$.
  \end{itemize}
\end{proof}

\section{Tait's method of computability}
\label{sec:0006}

\subsection{Synthetic computability}
\label{sec:0108}

For each type $A$, we assume two collections, $\neu{\Gamma}{A}$ and $\comp{\Gamma}{A}$, of terms indexed by contexts.
We may equivalently treat these collections as unary predicates, writing $\neu{\Gamma}{A}(t)$ to mean $t \in \neu{\Gamma}{A}$ and $\comp{\Gamma}{A}(t)$ \mutatismutandis.

\begin{axiom}\label{ax:0002}
  If $x : A \in \Gamma$, then $\neu{\Gamma}{A}(x)$.
\end{axiom}

\begin{axiom}\label{ax:0003}
  If $\comp{\Gamma}{A}(t)$ and $s \bRed t$, then $\comp{\Gamma}{A}(s)$.
\end{axiom}

\begin{axiom}\label{ax:0000}
  If $\neu{\Gamma}{A}(t)$, then $\comp{\Gamma}{A}(t)$, and if $\comp{\Gamma}{A}(t)$, then $\norm(t)$.
\end{axiom}

\begin{axiom}\label{ax:0001}
  If $\Gamma \vdash t : A$ and $\comp{\Delta}{\Gamma}(\gamma)$, then $\comp{\Delta}{A}(t[\gamma])$.
\end{axiom}

\begin{lemma}\label{thm:0010}
  For any context $\Gamma$, the identity substitution $\id{\Gamma}$ is computable.
\end{lemma}
\begin{proof}
  We proceed by induction on the length of $\Gamma$; the base case is trivial.
  In the induction case, suppose that $\Gamma = \Gamma',x : A$.
  We must show $\comp{\Gamma',x : A}{\Gamma',x : A}(\id{\Gamma',x : A})$.
  By the induction hypothesis, we have $\comp{\Gamma'}{\Gamma'}(\id{\Gamma'})$; hence it suffices to show that $\comp{\Gamma',x : A}{A}(x)$.
  This follows immediately from \cref{ax:0002,ax:0000}.
\end{proof}

\begin{theorem}[Normalization]\label{thm:0000}
  If $\Gamma \vdash t : A$, then $\norm(t)$.
\end{theorem}
\begin{proof}
  The identity substitution $\id{\Gamma}$ is computable by \cref{thm:0010}; hence $t[\id{\Gamma}] \equiv t$ is computable by \cref{ax:0001}.
  Then by \cref{ax:0000}, $t$ is normalizing.
\end{proof}

\subsection{Substantiating the computability structure}
\label{sec:0008}

\begin{definition}
  A \emph{neutral element} is either a variable or an eliminator whose major argument is a neutral element.
\end{definition}

\begin{construction}
  Normalizing neutral elements are generated by the following rules where $u$ ranges over neutral elements:
  \begin{mathpar}
    \ebrule{
      \hypo{x : A \in \Gamma}
      \infer1{\neu{\Gamma}{A}(x)}
    }\and
    \ebrule{
      \hypo{\norm(u\tpAnno{\tpEmp})}
      \infer1{\neu{\Gamma}{A}(\tmAbort{u})}
    }\and
    \ebrule{
      \hypo{\neu{\Gamma}{A \times B}(u)}
      \infer1{\neu{\Gamma}{A}(\tmPrl{u})}
    }\and
    \ebrule{
      \hypo{\neu{\Gamma}{A \times B}(u)}
      \infer1{\neu{\Gamma}{B}(\tmPrr{u})}
    }\and
    \ebrule{
      \hypo{\neu{\Gamma}{A + B}(u)}
      \hypo{\norm(c\tpAnno{C})}
      \hypo{\norm(c'\tpAnno{C})}
      \infer3{\neu{\Gamma}{C}(\tmCase{u}{c}{c'})}
    }\and
    \ebrule{
      \hypo{\neu{\Gamma}{A \to B}(u)}
      \hypo{\norm(a)}
      \infer2{\neu{\Gamma}{B}(\tmApp{u}{a})}
    }
  \end{mathpar}
\end{construction}

\begin{construction}
  Computable elements are generated by induction on types:
  \begin{itemize}
  \item[$\tpEmp$:] $\comp{\Gamma}{\tpEmp}(t)$ iff $\norm(t)$.
  \item[$A \times B$:] $\comp{\Gamma}{A \times B}(t)$ iff (1) $\comp{\Gamma}{A}(\tmPrl{t})$ and $\comp{\Gamma}{B}(\tmPrr{t})$, or (2) $t \bRed* \tmAbort{e}$ for some $e$ such that $\comp{\Gamma}{\tpEmp}(e)$.
  \item[$A + B$:] $\comp{\Gamma}{A + B}(t)$ iff (1) $t \bRed* \tmInl{a}$ for some $a$ such that $\comp{\Gamma}{A}(a)$, (2) $t \bRed* \tmInl{b}$ for some $b$ such that $\comp{\Gamma}{B}(b)$, (3) $t \bRed* t'$ for some $t'$ such that $\neu{\Gamma}{A + B}(t')$, or (4) $t \bRed* \tmAbort{e}$ for some $e$ such that $\comp{\Gamma}{\tpEmp}(e)$.
  \item[$A \to B$:] $\comp{\Gamma}{A \to B}(t)$ iff (1) for all $\Gamma \subseteq \Delta$, if $\comp{\Delta}{A}(a)$, then $\comp{\Delta}{B}(\tmApp{t}{a})$, or (2) $t \bRed* \tmAbort{e}$ for some $e$ such that $\comp{\Gamma}{\tpEmp}(e)$.
  \end{itemize}
\end{construction}

\begin{observation}\label{thm:0005}
  If $e\tpAnno{\tpEmp}$ is computable, then $\tmAbort{e}\tpAnno{A}$ is computable.
\end{observation}

\subsubsection{Verifying the axioms}
\label{sec:0009}

\begin{lemma}\label{thm:0002}
  If $\neu{\Gamma}{A}(t)$, then $\norm(t)$.
\end{lemma}
\begin{proof}
  Straightforward induction on the derivation of $\neu{\Gamma}{A}(t)$.
\end{proof}

\begin{lemma}\label{thm:1001}
  Let $\Gamma \subseteq \Delta$.
  If $\comp{\Gamma}{A}(t)$, then $\comp{\Delta}{A}(t)$.
  By extension, if $\comp{\Gamma}{\Pi}(\pi)$, then $\comp{\Delta}{\Pi}(\pi)$.
\end{lemma}
\begin{proof}
  Typing and $\beta$-reduction are stable under context weakening.
\end{proof}

\begin{theorem}[\cref{ax:0002}]\label{thm:0003}
  If $x : A \in \Gamma$, then $\neu{\Gamma}{A}(x)$.
\end{theorem}
\begin{proof}
  Immediate by definition.
\end{proof}

\begin{theorem}[\cref{ax:0003}]\label{thm:1010}
  If $\comp{\Gamma}{A}(t)$ and $s \bRed t$, then $\comp{\Gamma}{A}(s)$.
  Moreover, this theorem holds for $\bRed*$ by induction.
\end{theorem}
\begin{proof}
  Note that $\bRed*$ is transitive by definition; then the result follows by induction on $A$.
\end{proof}

\begin{theorem}[\cref{ax:0000}]\label{thm:0006}
  If $\neu{\Gamma}{A}(t)$, then $\comp{\Gamma}{A}(t)$, and if $\comp{\Gamma}{A}(t)$, then $\norm(t)$.
\end{theorem}
\begin{proof}
  We proceed by induction on $A$.
  \begin{itemize}
  \item[$\tpEmp$:] Suppose that $\neu{\Gamma}{\tpEmp}(t)$; then $t$ is normalizing by \cref{thm:0002}.
    Hence $\comp{\Gamma}{\tpEmp}(t)$ by definition.
    The other claim is immediate by definition.
  \item[$B \times C$:] Suppose that $\neu{\Gamma}{B \times C}(t)$; then we have $\neu{\Gamma}{B}(\tmPrl{t})$ and $\neu{\Gamma}{C}(\tmPrr{t})$ by definition.
    The induction hypothesis yields $\comp{\Gamma}{B}(\tmPrl{t})$ and $\comp{\Gamma}{C}(\tmPrr{t})$; hence $\comp{\Gamma}{B \times C}(t)$ by definition.
    
    Now suppose that $\comp{\Gamma}{B \times C}(t)$.
    There are two cases to consider: (1) $\comp{\Gamma}{B}(\tmPrl{t})$ and $\comp{\Gamma}{C}(\tmPrr{t})$, or (2) $t \bRed* \tmAbort{e}$ for some $e$ such that $\comp{\Gamma}{\tpEmp}(e)$.
    
    In the former case, we have $\norm(\tmPrl{t})$ and $\norm(\tmPrr{t})$ by the induction hypothesis; hence by a case analysis on the structure of $t$, we see that $t$ must be normalizing.
    In the latter case, since $\comp{\Gamma}{\tpEmp}(e)$, $e$ is normalizing by definition; hence $\tmAbort{e}$ is normalizing.
    Then it follows by the transitivity of $\bRed*$ that $t$ is normalizing.
    Note that this case is always trivial; hence we suppress it hereafter.
  \item[$B + C$:] Suppose that $\neu{\Gamma}{B + C}(t)$; then since $\bRed*$ is reflexive, $\comp{\Gamma}{B + C}(t)$ by definition.
    Now suppose that $\comp{\Gamma}{B + C}(t)$.
    There are three cases to consider; two of which are analogous to each other, so we show just one of them: (1) $t \bRed* \tmInl{a}$ for some $a$ such that $\comp{\Gamma}{A}(a)$, or (2) $t \bRed* t'$ for some $t'$ such that $\neu{\Gamma}{A + B}(t')$.

    In the former case, since $a$ is normalizing by the induction hypothesis, $\tmInl{a}$ is clearly normalizing.
    In the latter case, $t$ reduces to some normalizing neutral element $t'$, which is normalizing by \cref{thm:0002}; hence $t$ is normalizing by transitivity.
  \item[$B \to C$:] Suppose that $\neu{\Gamma}{B \to C}(t)$.
    Fix $\Gamma \subseteq \Delta$ and $\comp{\Delta}{B}(b)$; we must show that $\comp{\Delta}{C}(\tmApp{t}{b})$.
    By the induction hypothesis, $b$ is normalizing; hence $\neu{\Delta}{C}(\tmApp{t}{b})$ by definition, and the induction hypothesis gives $\comp{\Delta}{C}(\tmApp{t}{b})$.

    Now suppose that $\comp{\Gamma}{B \to C}(t)$.
    Choose $\Gamma \subseteq \Gamma,x : B$; note that since $\neu{\Gamma,x : B}{B}(x)$, $x$ is computable by the induction hypothesis.
    Then $\comp{\Gamma,x : B}{C}(\tmApp{t}{x})$ by definition.
    By the induction hypothesis, $\tmApp{t}{x}$ is normalizing.
    By a case analysis on the structure of $t$, we see that $t$ must be normalizing.
  \end{itemize}
\end{proof}

\begin{theorem}[\cref{ax:0001}]\label{thm:0302}
  If $\Gamma \vdash t : A$ and $\comp{\Delta}{\Gamma}(\gamma)$, then $\comp{\Delta}{A}(t[\gamma])$.
\end{theorem}
\begin{proof}
  We proceed by induction on the typing derivation $\Gamma \vdash t : A$.
  \begin{itemize}
  \item[\rVar:] Since $x[\gamma] \equiv \gamma(x)$, this is immediate by definition.
  \item[\rEmpE:] By the induction hypothesis, $\comp{\Delta}{\tpEmp}(e[\gamma])$; then $\tmAbort{e[\gamma]}$ is computable by \cref{thm:0005}.
  \item[\rProdI:] By the induction hypothesis, we have $\comp{\Delta}{B}(b[\gamma])$ and $\comp{\Delta}{C}(c[\gamma])$; hence $\tmPair{b[\gamma]}{c[\gamma]}$ is computable by definition and \cref{thm:1010}.
  \item[\rProdEl:] By the induction hypothesis, $p[\gamma]$ is computable.
    There are two cases to consider: (1) $\comp{\Delta}{B}(\tmPrl{p[\gamma]})$ and $\comp{\Delta}{C}(\tmPrr{p[\gamma]})$, or (2) $p[\gamma] \bRed* \tmAbort{e}$ for some $e$ such that $\comp{\Delta}{\tpEmp}(e)$.

    In the former case, $\tmPrl{p[\gamma]}$ is computable trivially.
    In the latter case, it suffices to show that $\tmPrl{\tmAbort{e}}$ is computable.
    Note that $e$ is normalizing by \cref{thm:0006}; hence $\tmAbort{e}$ is a normalizing neutral element by definition.
    Then $\tmPrl{\tmAbort{e}}$ is a normalizing neutral element by definition.
    Now apply \cref{thm:0006} again.
    Since this case is analogous for all other rules, we will omit it hereafter.
    \rProdEr{} is completely analogous.
  \item[\rSumIl:] By the induction hypothesis, $a[\gamma]$ is computable; hence $\tmInl{a[\gamma]}$ is computable by definition.
    \rSumIr{} is analogous.
  \item[\rSumE:] By the induction hypothesis, we have $\comp{\Delta}{D + E}(s[\gamma])$; there are three cases to consider, and two of which are analogous: (1) $s[\gamma] \bRed* \tmInl{d}$ for some computable $d$, or (2) $s[\gamma] \bRed* t'$ for some normalizing neutral element $t'$.

    In the former case, it suffices to show $\comp{\Delta}{C}(c[\gamma,d/x])$; this is immediate by the induction hypothesis since $\comp{\Delta}{\Gamma,x : D}(\gamma, d/x)$.
    In the latter case, note that variables are computable; then by the induction hypothesis, $c[\gamma,x/x]$ and $c'[\gamma,y/y]$ are computable and thus normalizing by \cref{thm:0006}.
    Then by definition, $\tmCase{s[\gamma]}{c[\gamma,x/x]}{c'[\gamma,y/y]}$ is a normalizing neutral element; hence it is computable by \cref{thm:0006}.
  \item[\rFunI:] Fix $\Delta \subseteq \Delta'$ and $b$ such that $\comp{\Delta'}{B}(b)$.
    It suffices to show $\comp{\Delta'}{C}(c[\gamma,b/y])$.
    By \cref{thm:1001}, $\comp{\Delta'}{\Gamma}(\gamma)$; hence $c[\gamma,b/y]$ is computable by the induction hypothesis.
  \item[\rFunE:] By the induction hypothesis, $f[\gamma]$ and $b[\gamma]$ are computable; hence $\tmApp{f[\gamma]}{b[\gamma]}$ is computable by definition.
  \end{itemize}
\end{proof}

\section{Conclusion}
\label{sec:0007}



\bibliography{bib}
\bibliographystyle{alpha}

\end{document}
